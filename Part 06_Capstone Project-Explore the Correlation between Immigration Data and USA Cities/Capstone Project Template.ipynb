{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import glob\n",
    "import configparser\n",
    "import boto3\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "from pyspark.sql.functions import col, countDistinct, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pandas dataframe display parameters\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth',100)\n",
    "pd.set_option('max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('decp.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.0.0\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\",\"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\",\"true\") \\\n",
    "        .config(\"dfs.client.read.shortcircuit.skip.checksum\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Scope\n",
    "**What Data**\n",
    "\n",
    "The main dataset include data on immigration to the United States, and supplementary datasets will include data on airport codes, U.S. city demographics, and temperature data. \n",
    "\n",
    "**What Plan to Do**\n",
    "\n",
    "We will use these data to build a model to explore the correlation between immigration data and month, immigration reason.\n",
    "\n",
    "**What looks like** \n",
    "\n",
    "![immigration_reason](./image/immigration_reason.png)\n",
    "\n",
    "![immigration_reason_month](./image/immigration_reason_month.png)\n",
    "\n",
    "**What Tools**\n",
    "\n",
    "Pandas, Numpy, Pyspark, Matplotlib, etc.\n",
    "\n",
    "### 1.2 Describe and Gather Data\n",
    "\n",
    "### 1.2.1 Describe\n",
    "#### 1.  I94 Immigration Data & Descriptions\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://www.trade.gov/national-travel-and-tourism-office) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "\n",
    "Some of files\n",
    "Sample : *immigration_data_sample.csv*\n",
    "Descriptions : *I94_SAS_Labels_Descriptions.SAS*\n",
    "\n",
    "#### 2. World Temperature Data\n",
    "This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "File : *GlobalLandTemperaturesByCity.csv*\n",
    "\n",
    "#### 3. U.S. City Demographic Data\n",
    "This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "\n",
    "File : *us-cities-demographics.csv*\n",
    "\n",
    "#### 4. Airport Code Table\n",
    "This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "File : *airport-codes_csv.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Gather Data\n",
    "#### 1. I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "# read the path of all immigration data\n",
    "fpath=\"./data/18-83510-I94-Data-2016/\"\n",
    "immigration_files=glob.glob(fpath +'*.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the immigration files and save date to parquet\n",
    "for i,f in enumerate(immigration_files):\n",
    "    if i > 0:\n",
    "        print(f)\n",
    "#       df_immigration = pd.read_sas(f, 'sas7bdat', encoding='ISO-8859-1', chunksize=20)\n",
    "        df_immigration = pd.read_sas(f, 'sas7bdat', encoding='ISO-8859-1')\n",
    "#         print(df_immigration.head())\n",
    "        df_immigration.to_parquet('./img_parq_temp/df_immigration_' + f[-18:] + '.parquet')\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because it takes too long to store immigration data，so we directly read the previously stored data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because it takes too long to store immigration data，\n",
    "# so we directly read the previously stored data.\n",
    "df_immigration = spark.read.parquet('./immig_parq_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. I94 Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file path\n",
    "bucket_name = './source/'\n",
    "city_files = os.path.join(bucket_name,'I94_SAS_Labels_Descriptions.SAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "\n",
    "def code_mapper(f_content, idx):\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic\n",
    "with open(city_files) as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '') \n",
    "    \n",
    "i94cit_res = code_mapper(f_content, \"i94cntyl\")\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94visa = {'1':'Business',\n",
    "            '2': 'Pleasure',\n",
    "            '3' : 'Student'}\n",
    "\n",
    "df_i94cit_res= pd.DataFrame(list(i94cit_res.items()),columns = ['i94cit_res','origin_travelled_country'])\n",
    "\n",
    "df_i94port= pd.DataFrame(list(i94port.items()),columns = ['i94port','destination_city'])\n",
    "\n",
    "df_i94mode= pd.DataFrame(list(i94mode.items()),columns = ['i94mode','transportation'])\n",
    "\n",
    "df_i94addr= pd.DataFrame(list(i94addr.items()),columns = ['i94addr','state'])\n",
    "\n",
    "df_i94visa= pd.DataFrame(list(i94visa.items()),columns = ['i94visa','immigration_reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Airport Code Table, World Temperature Data, U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file path\n",
    "airpot_files = 'airport-codes_csv.csv'\n",
    "temperature_files = 'GlobalLandTemperaturesByCity.csv'\n",
    "demographic_files = 'us-cities-demographics.csv'\n",
    "\n",
    "# bucket_name = 's3://lcw-udacity-capstone/source/'\n",
    "bucket_name = './source/'\n",
    "airport_files = os.path.join(bucket_name, airpot_files)\n",
    "temperature_files = os.path.join(bucket_name, temperature_files)\n",
    "demographic_files = os.path.join(bucket_name, demographic_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "df_airport = pd.read_csv(airport_files, sep=',')\n",
    "df_temperature = pd.read_csv(temperature_files, sep=',')\n",
    "df_demographic = pd.read_csv(demographic_files,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Convert data format\n",
    "df_immigration = df_immigration.withColumn(\"cicid\", df_immigration[\"cicid\"].astype('bigint'))\n",
    "df_immigration = df_immigration.withColumn(\"i94yr\", df_immigration[\"i94yr\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"i94mon\", df_immigration[\"i94mon\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"i94cit\", df_immigration[\"i94cit\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"i94res\", df_immigration[\"i94res\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"arrdate\", df_immigration[\"arrdate\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"i94mode\", df_immigration[\"i94mode\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"depdate\", df_immigration[\"depdate\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"i94bir\", df_immigration[\"i94bir\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"i94visa\", df_immigration[\"i94visa\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"count\", df_immigration[\"count\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"dtadfile\", df_immigration[\"dtadfile\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"biryear\", df_immigration[\"biryear\"].cast(IntegerType()))\n",
    "df_immigration = df_immigration.withColumn(\"admnum\", df_immigration[\"admnum\"].astype('bigint'))\n",
    "df_immigration = df_immigration.withColumn(\"depdate\", df_immigration[\"depdate\"].astype('int'))\n",
    "\n",
    "# transform the date data and extract columns to create table\n",
    "df_immigration = df_immigration.selectExpr(\"*\", \"date_add('1960-01-01',arrdate) as arrival_date\",\\\n",
    "                     \"date_add('1960-01-01',depdate) as departure_date\").drop('arrdate','depdate')\n",
    "\n",
    "df_immigration = df_immigration.sort('i94yr','i94mon','cicid')\n",
    "\n",
    "# the \"cicid\" column be set as the primary key and be auto-incrementing\n",
    "df_immigration = df_immigration.withColumn(\"cicid\", monotonically_increasing_id())\n",
    "\n",
    "# extract columns to create table\n",
    "immigration_table = df_immigration.select('cicid', 'i94yr',   'i94mon',  'i94cit',  'i94res',  'i94port',  'arrival_date',  \n",
    "                                        'departure_date', 'i94mode',  'i94addr',   'i94visa','matflag', 'airline',  'admnum',  'fltno')\n",
    "immigrant_table = df_immigration.select('admnum', 'i94bir', 'gender', 'biryear', 'visatype').dropDuplicates()\n",
    "\n",
    "# To filter out immigrant_table who has been recorded by immigration for many times,\n",
    "# but the information recorded before and after is inconsistent\n",
    "admnum_error = immigrant_table.groupby('admnum').count().orderBy(col(\"count\").desc()).where('count > 1')\n",
    "pd_admnum_error = admnum_error.toPandas()\n",
    "rows = pd_admnum_error['admnum'].tolist()\n",
    "\n",
    "# To filter out the immigration_table what has the error Admission Number\n",
    "immigration_table =  immigration_table.filter(~col('admnum').isin(rows))\n",
    "\n",
    "# To filter out the immigrant_table what has the error Admission Number\n",
    "immigrant_table = immigrant_table.filter(~col('admnum').isin(rows))\n",
    "\n",
    "immigration_table = immigration_table.fillna(0)\n",
    "immigrant_table = immigrant_table.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. I94 Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only df_i94port need to clean\n",
    "df_i94port['city'] = df_i94port['destination_city'].str.split(',', expand=True)[0]\n",
    "df_i94port['region'] = df_i94port['destination_city'].str.split(',', expand=True)[1]\n",
    "df_i94port.drop(['destination_city'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Airport Code Table, World Temperature Data, U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter the closed airports\n",
    "airport_table = df_airport[~df_airport['type'].isin(['closed'])]\n",
    "\n",
    "# To filter the airports in US\n",
    "airport_table = airport_table[airport_table['iso_country']=='US']\n",
    "\n",
    "airport_table = airport_table[~airport_table['local_code'].isna()]\n",
    "\n",
    "airport_table['longitude'] = airport_table['coordinates'].str.split(',', expand=True)[0].astype('float').round(2)\n",
    "airport_table['latitude'] = airport_table['coordinates'].str.split(',', expand=True)[1].astype('float').round(2)\n",
    "airport_table['iso_region'] = airport_table['iso_region'].str.split('-', expand=True)[1]\n",
    "\n",
    "airport_table.drop(['coordinates','continent','iata_code'], axis=1, inplace=True)\n",
    "\n",
    "# drop duplicates of local_code, and keep the first\n",
    "airport_table = airport_table.drop_duplicates(subset=['local_code'], keep='first', inplace=False)\n",
    "\n",
    "airport_table = airport_table.reset_index()\n",
    "airport_table.rename(columns = {'index':'airport_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter out the rows what have null value\n",
    "temperature_table = df_temperature[~df_temperature['AverageTemperature'].isna()]\n",
    "\n",
    "# transport the type of 'dt' to <datetime>\n",
    "temperature_table.loc[:,'dt'] = pd.to_datetime(temperature_table.loc[:,'dt'])\n",
    "\n",
    "# In view of the impact of human activities on climate in recent years, we screened the data after 1985 \n",
    "# to calculate the average temperature\n",
    "temperature_table = temperature_table[(temperature_table['dt']>'1985-12-31')]\n",
    "\n",
    "temperature_table['month'] = temperature_table['dt'].dt.month\n",
    "\n",
    "# delete the letter E what in the columns of 'latitude' and 'longitude'\n",
    "temperature_table.loc[:,'Latitude'] = temperature_table.apply(lambda x: x['Latitude'][:-1], axis = 1)\n",
    "temperature_table.loc[:,'Longitude'] = temperature_table.apply(lambda x: x['Longitude'][:-1], axis = 1)\n",
    "\n",
    "temperature_table['Longitude'] = '-' + temperature_table['Longitude'] \n",
    "\n",
    "temperature_table['Latitude'] = temperature_table['Latitude'].astype('float')\n",
    "temperature_table['Longitude'] = temperature_table['Longitude'].astype('float')\n",
    "\n",
    "# calculate the mean of city's temperature of each month\n",
    "temperature_table_month=temperature_table.groupby(['Country','City','month','Latitude','Longitude'],as_index=False)['AverageTemperature'].mean().round(3)\n",
    "\n",
    "temperature_table_month = temperature_table_month.reset_index()\n",
    "temperature_table_month.rename(columns = {'index':'temperature_id'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter out the columns of 'State Code, Race, Count', and merge the duplicates\n",
    "demographic_table = df_demographic.iloc[:,0:9].drop_duplicates()\n",
    "demographic_table = demographic_table.fillna(0)\n",
    "demographic_table['Male Population'] = demographic_table['Male Population'].astype('int')\n",
    "demographic_table['Female Population'] = demographic_table['Female Population'].astype('int')\n",
    "demographic_table['Number of Veterans'] = demographic_table['Number of Veterans'].astype('int')\n",
    "demographic_table['Foreign-born'] = demographic_table['Foreign-born'].astype('int')\n",
    "\n",
    "demographic_table = demographic_table.reset_index()\n",
    "demographic_table.rename(columns = {'index':'demographic_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. When we have processed the data, we write them to the PARQUET, CSV file for using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write table to parquet files \n",
    "output_data = './transformed/'\n",
    "immigration_table.repartition(4).write.parquet(output_data +'immigration.parq', mode=\"overwrite\")\n",
    "immigrant_table.repartition(1).write.parquet(output_data +'immigrant.parq', mode=\"overwrite\")\n",
    "\n",
    "# write table to csv files\n",
    "s3_output_data = './transformed/'\n",
    "df_i94cit_res.to_csv(s3_output_data + 'df_i94cit_res.csv',index=False)\n",
    "df_i94port.to_csv(s3_output_data + 'df_i94port.csv',index=False)\n",
    "df_i94mode.to_csv(s3_output_data + 'df_i94mode.csv',index=False)\n",
    "df_i94addr.to_csv(s3_output_data + 'df_i94addr.csv',index=False)\n",
    "df_i94visa.to_csv(s3_output_data + 'df_i94visa.csv',index=False)\n",
    "\n",
    "airport_transformed = os.path.join(s3_output_data, 'us_airport.csv')\n",
    "airport_table.to_csv(airport_transformed, index=False)\n",
    "temperature_table_month.to_csv(s3_output_data + 'temperature_table_month.csv',index=False)\n",
    "demographic_table.to_csv(s3_output_data + 'demographic_table.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Schema for Immigration Analysis\n",
    "![schema](image/immigration.png)\n",
    "\n",
    "- In this model, we don't consider the violet tables, like *i94cit_res, i94mode, temperature, demographic*. Otherwise, the analysis based on the fact table will be greatly affected.\n",
    "\n",
    "- If we want to explore the relationship between temperature, urban population and immigration data, we can build another model\n",
    "\n",
    "#### Fact Table\n",
    "immigration - records with air transportation of immigrantion\n",
    "- *cicid, year, month, airport_id, state_id, city, visa_id, admnum, longitude, latitude*\n",
    "\n",
    "#### Dimension Table\n",
    "i94port - code of destination city\n",
    "\n",
    "i94addr - is where the immigrants resides in USA \n",
    "\n",
    "i94visa - reason for immigration\n",
    "\n",
    "airport - information of airports in USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Steps\n",
    "1. Create tables (**create_tables.py**)\n",
    "2. Read data, explore and clean data, save data (**etl.py**)\n",
    "3. Copy data to PostgreSQL (**airflow/dags/dag.py**)\n",
    "4. Insert to fact table (**airflow/dags/dag.py**)\n",
    "5. Analyse the fact table (**immigration_analysis.ipynb**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\"airflow/dags/dag.py\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **\"airflow/dags/dag.py\"**\n",
    "\n",
    "run_quality_checks = DataQualityOperator(\n",
    "    task_id='Run_data_quality_checks',\n",
    "    dag=dag,\n",
    "    dq_checks=[\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM immigration WHERE cicid is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM immigrant WHERE admnum is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM i94cit_res WHERE i94cit_res is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM i94port WHERE i94port is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM i94mode WHERE i94mode is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM i94addr WHERE i94addr is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM i94visa WHERE i94visa is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM airport WHERE local_code is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM temperature WHERE averagetemperature is null\", 'expected_result':0},\n",
    "        {'check_sql': \"SELECT COUNT(*) FROM demographic WHERE total_population is null\", 'expected_result':0}\n",
    "    ],\n",
    "    table=('immigration', 'immigrant', 'i94cit_res', 'i94port', 'i94mode', 'i94addr', 'i94visa', 'airport', 'temperature', 'demographic'),\n",
    "    redshift_conn_id=\"redshift\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\"airflow/plugins/operators/data_quality.py\"**\n",
    "The operator's main functionality is to receive one or more SQL based test cases along with the expected results and execute the tests. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Data_Dictionary.xlsx\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. tools and technologies\n",
    "\n",
    "Pandas - We use it to read and wirte files to dataframe.\n",
    "\n",
    "Numpy, Matplotlib - We can use them to analyse the immigration and plot.\n",
    "\n",
    "Pyspark - When we read a large amount of data, pandas is no longer applicable. At this time, we need to use pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n",
    "\n",
    "Monthly. Because most of our analysis is done on a monthly basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "- The data was increased by 100x.\n",
    "\n",
    "When the amount of data is small, we can use the traditional way to process it on the personal computer. Due to the scalability of redshift, we can also directly use redshift to store data, so that even if the data was increased by 100x, we don't have to worry about insufficient storage space, and use Hadoop cluster to calculate and analyze.\n",
    "\n",
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "Airflow is a programmable, scheduling and monitoring workflow platform.  Airflow provides rich command-line tools for system control, and its web management interface can also facilitate the control and scheduling of tasks, and real-time monitor the operation status of tasks. We can use airflow to schedule the execution time of tasks.\n",
    "\n",
    "- The database needed to be accessed by 100+ people.\n",
    "\n",
    "Traditional relational database can be contentedly used. However, in order to improve the access and reading efficiency, we can choose NoSQL, such as Cassandra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
