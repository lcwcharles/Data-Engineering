# Data Pipelines with Airflow
The Project of Data Engineering Nanodegree Program on the Udacity.

# Introduction
A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

To complete the project, we will need to create our own custom operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step.

# Project Datasets

## Song Dataset
Source: **s3://udacity-dend/song_data**

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

*song_data/A/B/C/TRABCEI128F424C983.json*

*song_data/A/A/B/TRAABJL12903CDCF1A.json*

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

*{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}*

## Log Dataset
Source: **s3://udacity-dend/log_data**

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.

*log_data/2018/11/2018-11-12-events.json*

*log_data/2018/11/2018-11-13-events.json*

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.
![2018-11-12-events.json](image/log-data.png)

# Project Instructions
## Schema for Song Play Analysis
### Fact Table
 1. songplays - records in log data associated with song plays i.e. records with page **NextSong**
- *playid, start_time, userid, level, songid, artistid, sessionid, location, user_agent*
### Dimension Tables
1. users - users in the app
  - *user_id, firstname, lastname, gender, level*
2. songs - songs in music database
  - *song_id, title, artist_id, year, duration*
3. artists - artists in music database
  - *artist_id, artist_name, artist_location, artist_lattitude, artist_longitude*
4. time - timestamps of records in songplays broken down into specific units
  - *start_time, hour, day, week, month, year, weekday*

## Project Template
1.**create_tables.sql**  creates the fact table(songplays) and the dimension tables(users, songs, artists, time).

2.**helpers/sql_queries.py** provides a helper class that contains all the SQL transformations.

3.**operators/data_quality.py** is used to run checks on the data itself. The operator's main functionality is to receive one or more SQL based test cases along with the expected results and execute the tests. 

4.**operators/load_dimension.py \ operators/load_fact.py** With dimension and fact operators, we can utilize the SQL helper(**helpers/sql_queries.py**) class to run data transformations.

5.**operators/stage_redshift.py** is able to load any JSON formatted files from S3 to Amazon Redshift. 

6.**README.md** provides discussion on our process and decisions.
